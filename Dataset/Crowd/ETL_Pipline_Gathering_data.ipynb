{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d8243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import holidays as hl\n",
    "\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max_columns',None)\n",
    "OUTDIR = \"data_weather/Final\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "import openmeteo_requests\n",
    "\n",
    "import requests_cache\n",
    "from retry_requests import retry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4067a9",
   "metadata": {},
   "source": [
    "### Need cache for Open Meto from Api Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c6ebcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Open-Meteo API client with cache and retry on error # <--- this is from Open Meteo Api Docs\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed36682a",
   "metadata": {},
   "source": [
    "### Load in Popular POI's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ad4c0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location_Name</th>\n",
       "      <th>Type_of_Attraction</th>\n",
       "      <th>Attraction_Category</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CN Tower</td>\n",
       "      <td>Tower</td>\n",
       "      <td>Urban Landmark</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>Canada</td>\n",
       "      <td>43.6426</td>\n",
       "      <td>-79.3871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Location_Name Type_of_Attraction Attraction_Category     City Country  \\\n",
       "0      CN Tower              Tower      Urban Landmark  Toronto  Canada   \n",
       "\n",
       "   Latitude  Longitude  \n",
       "0   43.6426   -79.3871  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GB = pd.read_csv('data_weather/Global_Tourist_Attractions.csv')\n",
    "GB.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb7c518",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "- Using Open Meto to get past weather data, code is from their APi doc for Historical Forecast \n",
    "- As of Nov 3 2025 Nov 1 2025 is the latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4a8390cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weather_Requester(lat:float,long:float,stDate:str,edDate:str) -> pd.DataFrame:\n",
    "    url = \"https://historical-forecast-api.open-meteo.com/v1/forecast\"\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": long,\n",
    "        \"start_date\": stDate,\n",
    "        \"end_date\": edDate,\n",
    "        \"daily\": [\"temperature_2m_mean\", \"wind_speed_10m_mean\", \"precipitation_sum\", \"relative_humidity_2m_mean\"],\n",
    "        \"timezone\": \"America/New_York\"\n",
    "    }\n",
    "    response = openmeteo.weather_api(url,params=params)\n",
    "    \n",
    "    dly = response[0].Daily()\n",
    "   \n",
    "    dT = dly.Variables(0).ValuesAsNumpy()\n",
    "    dW = dly.Variables(1).ValuesAsNumpy()\n",
    "    dP = dly.Variables(2).ValuesAsNumpy()\n",
    "    dH = dly.Variables(3).ValuesAsNumpy()\n",
    "\n",
    "    daily_data = {\"Date\": pd.date_range(\n",
    "\tstart = pd.to_datetime(dly.Time(), unit = \"s\", utc = True),\n",
    "\tend =  pd.to_datetime(dly.TimeEnd(), unit = \"s\", utc = True),\n",
    "\tfreq = pd.Timedelta(seconds = dly.Interval()),\n",
    "\tinclusive = \"left\"\n",
    "    )}\n",
    "\n",
    "    daily_data['Date'] = daily_data['Date'].date\n",
    "\n",
    "    daily_data['Weather_Temperature_Avg'] = dT\n",
    "    daily_data['Weather_Wind_Speed_Avg'] = dW\n",
    "    daily_data['Weather_Precipitation_Sum'] = dP\n",
    "    daily_data['Weather_Relative_Humidity_Avg'] = dH\n",
    "\n",
    "    daily_dataframe = pd.DataFrame(data = daily_data)\n",
    "\n",
    "    daily_dataframe['Date'] = daily_dataframe['Date'].apply(lambda x: str(x))\n",
    "    \n",
    "    return daily_dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb4c1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adding_Holiday(df:pd.DataFrame,country:str) -> pd.DataFrame:\n",
    "    df['Holiday'] = df['Date'].apply(lambda x: 1 if hl.country_holidays(country=country).get(x) != None else 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8892070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Handle_Date(df:pd.DataFrame) -> pd.DataFrame:    \n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].apply(lambda x: parser.parse(x).strftime('%Y-%m-%d'))\n",
    "        df.rename(columns={col:'Date'}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdaea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adding_time_features(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    seasons = {1:'Winter',2:'Spring',3:'Spring',4:'Spring',5:'Summer',6:'Summer',7:'Summer',8:'Fall',9:'Fall',10:'Fall',11:'Winter',12:'Winter'}\n",
    "    month = {1:'Janary',2:'February',3:'March',4:'Apirl',5:'May',6:'June',7:'July',8:'August',9:'Setember',10:'October',11:'November',12:'December'}\n",
    "    df['Month'] = df['Date'].apply(lambda x: parser.parse(x).month)\n",
    "    df['Season'] = df['Month'].apply(lambda x: seasons.get(x))\n",
    "    df['Month'] = df['Month'].apply(lambda x: month.get(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e04c2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Attract_Scre_Tour_Sat_Level(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    # Base attraction score by city (0-1 scale)\n",
    "    city_score = {\"Toronto\": 0.8,\"Edmonton\": 0.55,\"Sydney\": 0.85,\"Melbourne\": 0.7,\"Auckland\": 0.60,\"Dublin\": 0.65}\n",
    "\n",
    "    # Map each city’s base score\n",
    "    df[\"Attraction_Score\"] = df[\"City\"].map(city_score)\n",
    "\n",
    "    # Add small random variation (so it's not static)\n",
    "    df[\"Attraction_Score\"] = (0.4*df['Avg_Daily_Pedestrian_Count'] + 0.3*df['Weather_Temperature_Avg'] + 0.1*df['Weather_Precipitation_Avg'] + 0.2*df['Weather_Wind_Speed_Avg'])/1000\n",
    "    #(df[\"Attraction_Score\"] + np.random.uniform(-0.05, 0.05, len(df))).clip(0, 1).round(3)\n",
    "\n",
    "    # Tourist_Saturation_Level = normalized Pedestrian_Avg (0–1)\n",
    "    # df[\"Tourist_Saturation_Level\"] = df.groupby(\"City\")[\"Avg_Daily_Pedestrian_Count\"].transform(lambda s: (s - s.min()) / (s.max() - s.min())).round(3)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72fa1537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfsComb(dfs:pd.DataFrame,wB:list[str]) -> pd.DataFrame:\n",
    "    U_cols = []\n",
    "    for df in dfs: \n",
    "        r = Handle_Date(df)\n",
    "        if type(r) == str: \n",
    "            return r\n",
    "        df = r \n",
    "        for cn in df.columns:\n",
    "            U_cols.append(cn)\n",
    "\n",
    "    for cn in list(set(U_cols)):\n",
    "        if U_cols.count(cn) != len(dfs):\n",
    "            U_cols = [v for v in U_cols if v != cn]\n",
    "    \n",
    "    U_cols = list(set(U_cols))\n",
    "\n",
    "    dfs_comb = pd.DataFrame(columns=U_cols)\n",
    "    for df in dfs:\n",
    "        dfs_comb = pd.concat([dfs_comb,df[U_cols]],axis=0)    \n",
    "    \n",
    "    CNameChk = wB \n",
    "    if len(CNameChk) > 0:\n",
    "        drpC = []\n",
    "        for cn in dfs_comb.columns:\n",
    "            if sum([1 if chk in cn else 0 for chk in CNameChk]) != 0: \n",
    "                drpC.append(cn)\n",
    "        dfs_comb = dfs_comb.drop(columns=drpC)\n",
    "    dfs_comb.insert(0,'Date',dfs_comb.pop('Date'))\n",
    "    dfs_comb.dropna(axis=1,how='all')\n",
    "    NanRation_PerCol = zip(dfs_comb.columns,dfs_comb.isna().sum()/dfs_comb.shape[0])\n",
    "    for valuepair in NanRation_PerCol:\n",
    "        if valuepair[1] > 0.15:\n",
    "            dfs_comb = dfs_comb.drop(columns=[valuepair[0]])\n",
    "    dfs_comb = dfs_comb.fillna(0.0)\n",
    "    \n",
    "    return dfs_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b7f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfsCombo_Final(dfs_comb:pd.DataFrame,U_cols:list[str],Country:str,Tol:float,IntsecLatLongs:dict[str, list[float]],Location_ID:str) -> pd.DataFrame:\n",
    "    avg_col = U_cols[1:]\n",
    "    dfs_comb_daily = dfs_comb.groupby(['Date'], as_index=False)[avg_col].sum()\n",
    "    dfs_comb_daily = dfs_comb_daily[dfs_comb_daily['Date'] >= '2021-01-01'].reset_index(drop=True)\n",
    "\n",
    "    GB_NZ = GB[GB['Country'] == Country].reset_index(drop=True)\n",
    "    \n",
    "    tolerance = Tol\n",
    "    dfs_comb_gbLocs = pd.DataFrame(columns=['Country','City','Location_ID','Location_Name','Type_of_Attraction','Attraction_Category',\n",
    "                                            'Latitude','Longitude','Date','Avg_Daily_Pedestrian_Count'])\n",
    "    for i,L in GB_NZ.iterrows():\n",
    "        Lat,Long = L['Latitude'],L['Longitude']\n",
    "        Ints = [key for key, value in IntsecLatLongs.items() if (value[0] - Lat) <= tolerance and (value[1] - Long) <= tolerance]\n",
    "        if not Ints:continue\n",
    "        Davg = dfs_comb_daily[Ints].sum(axis=1)\n",
    "        d =  dfs_comb_daily['Date']\n",
    "        for r in zip(Davg,d):\n",
    "            dfs_comb_gbLocs.loc[len(dfs_comb_gbLocs)] = {\n",
    "                'Country':L['Country'],'City':L['City'],'Location_ID':f'{Location_ID}_{i+1}','Location_Name':L['Location_Name'],'Type_of_Attraction':L['Type_of_Attraction'],\n",
    "                'Attraction_Category':L['Attraction_Category'],'Latitude':Lat,'Longitude':Long,'Date':r[1],'Avg_Daily_Pedestrian_Count':r[0]}\n",
    "    \n",
    "    return dfs_comb_gbLocs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b58f11f",
   "metadata": {},
   "source": [
    "### Getting Dublin Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "37ddd3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dublin():\n",
    "    Dublin_latlong = {\n",
    "        \"O'Connell St/Parnell St/AIB\":[53.347861,-6.262075],\n",
    "        \"D'olier st/Burgh Quay\":[53.346362,-6.258316],\n",
    "        \"College Green/Church Lane\":[53.344356,-6.260970],\n",
    "        \"Grafton Street / Nassau Street / Suffolk Street\":[53.343208,-6.259262],\n",
    "        \"Henry Street/Coles Lane/Dunnes\":[53.3501148,-6.2641621],\n",
    "        \"Phibsborough Rd/Enniskerry Road\":[53.363647,-6.272056],\n",
    "        \"Grand Canal st upp/Clanwilliam place\":[53.341236,-6.240578],\n",
    "        \"Baggot st upper/Mespil rd/Bank\":[53.334004,-6.245193],\n",
    "        \"Grafton Street/CompuB\":[53.340153,-6.260714],\n",
    "        \"Mary st/Jervis st\":[53.348774,-6.266618],\n",
    "        \"Capel st/Mary street\":[53.348477,-6.268733],\n",
    "        \"College Green/Bank Of Ireland\":[53.344397,-6.260329]         \n",
    "    } # These all exist in all dfs column headers\n",
    "    folder = './data_weather/Dublin'\n",
    "    dfs = [pd.read_csv(os.path.join(folder,f)) for f in os.listdir(folder) if f.endswith('.csv')]\n",
    "    dfs_comb = dfsComb(dfs,['North','East','South','West','Inbound','inbound','Outbound','outbound','IN','OUT',\n",
    "                'Pedestrian','Pedestrians','place/Google','Channel','Peds','1','2','old',\n",
    "                '(','PYRO EVO Temporary Counter']) # Obtained from trail and error runs \n",
    "\n",
    "    df_F = dfsCombo_Final(dfs_comb,dfs_comb.columns.tolist(),'Ireland',0.0285,Dublin_latlong,'IRDUB')\n",
    "    df_F = adding_Holiday(df_F,'IE')\n",
    "\n",
    "    df_F = df_F[df_F['Date'] <= '2025-11-08'].reset_index(drop=True) # Open Meto has a day limit on latest day   \n",
    "     \n",
    "    gp = df_F.groupby(['Latitude','Longitude'],as_index=False)['Date'].agg(['min', 'max'])\n",
    "\n",
    "    gpC = pd.DataFrame()\n",
    "    for i,r in gp.iterrows():\n",
    "        Wth = Weather_Requester(r['Latitude'],r['Longitude'],r['min'],r['max'])\n",
    "        Wth.insert(0, 'Longitude', float(r['Longitude']))\n",
    "        Wth.insert(0, 'Latitude', float(r['Latitude']))\n",
    "        gpC = pd.concat((gpC,Wth),axis='index',).reset_index(drop=True)\n",
    "    \n",
    "    M = pd.merge(df_F,gpC,on=['Latitude','Longitude','Date'],how='outer').dropna(how='any')\n",
    "\n",
    "    M.to_csv(f\"{OUTDIR}/Dublin_Pedestrian_Hourly.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ba8499",
   "metadata": {},
   "source": [
    "### Getting Auckland Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2199eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auckland():\n",
    "    NewZe_latlong= {\n",
    "        '107 Quay Street':[-36.84294,174.7657151],\n",
    "        'Te Ara Tahuhu Walkway':[-36.8445354,174.7689804],\n",
    "        'Commerce Street West':[-37.7924771,175.2788845],\n",
    "        '7 Custom Street East':[-36.84518,174.76742],\n",
    "        '45 Queen Street':[-36.845001,174.766266],\n",
    "        '30 Queen Street':[-36.8485,174.7633],\n",
    "        '19 Shortland Street':[-36.84495,174.766575],\n",
    "        '2 High Street':[-36.8496,174.7644],\n",
    "        '1 Courthouse Lane':[36.8435,174.7638],\n",
    "        '61 Federal Street':[-36.8474453,174.7577998],\n",
    "        '59 High Street':[-36.8487668,174.7612574],\n",
    "        '210 Queen Street':[-36.848873,174.765435],\n",
    "        '205 Queen Street':[-36.8492249,174.7643553],\n",
    "        '8 Darby Street EW':[-36.8496018,174.7640929],\n",
    "        '8 Darby Street NS':[-36.8496018,174.7640929],\n",
    "        '261 Queen Street':[-36.8504686,174.7643253],\n",
    "        '297 Queen Street':[-36.8516857,174.7615011],\n",
    "        '150 K Road':[-36.857909,174.7600514],\n",
    "        '183 K Road':[-36.8574364,174.7576088],\n",
    "        } # These all exist in all dfs column headers\n",
    "    folder = './data_weather/Auckland Data'\n",
    "    dfs = [pd.read_csv(os.path.join(folder,f)) for f in os.listdir(folder) if f.endswith('.csv')]\n",
    "    Temp = []\n",
    "    for df in dfs:\n",
    "        df = df.drop(columns=['Time'])\n",
    "        if df['150 K Road'].dtypes == object:\n",
    "            df['150 K Road'] = df['150 K Road'].apply(lambda x: 0.0 if type(x) != int and type(x) != float else x) # coerce invalid parsing --> NaN then --> 0.0  \n",
    "        Temp.append(df)\n",
    "    dfs = Temp\n",
    "    dfs_comb = dfsComb(dfs,[\n",
    "\n",
    "    ]) #  Left blank as the unessary columns will be dropped with out it\n",
    "    \n",
    "    df_F = dfsCombo_Final(dfs_comb,dfs_comb.columns.tolist(),'New Zealand',0.0285,NewZe_latlong,'NZAUK')\n",
    "    df_F = adding_Holiday(df_F,'NZ')\n",
    "\n",
    "    df_F = df_F[df_F['Date'] <= '2025-11-08'].reset_index(drop=True) # Open Meto has a day limit on latest day   \n",
    "     \n",
    "    gp = df_F.groupby(['Latitude','Longitude'],as_index=False)['Date'].agg(['min', 'max'])\n",
    "\n",
    "    gpC = pd.DataFrame()\n",
    "    for i,r in gp.iterrows():\n",
    "        Wth = Weather_Requester(r['Latitude'],r['Longitude'],r['min'],r['max'])\n",
    "        Wth.insert(0, 'Longitude', float(r['Longitude']))\n",
    "        Wth.insert(0, 'Latitude', float(r['Latitude']))\n",
    "        gpC = pd.concat((gpC,Wth),axis='index',).reset_index(drop=True)\n",
    "    \n",
    "    M = pd.merge(df_F,gpC,on=['Latitude','Longitude','Date'],how='outer').dropna(how='any')\n",
    "\n",
    "    M.to_csv(f\"{OUTDIR}/Auckland_Pedestrian_Hourly.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c19642",
   "metadata": {},
   "source": [
    "### Running all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "762f8be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\athar\\AppData\\Local\\Temp\\ipykernel_29240\\860823027.py:34: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dfs_comb = dfs_comb.fillna(0.0)\n",
      "C:\\Users\\athar\\AppData\\Local\\Temp\\ipykernel_29240\\860823027.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dfs_comb = pd.concat([dfs_comb,df[U_cols]],axis=0)\n"
     ]
    }
   ],
   "source": [
    "auckland()\n",
    "dublin()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
